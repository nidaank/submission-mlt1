# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BfvLHVJ_9Xr7x7e1B2PsbxluhrFZU80b

## Import Library

Melakukan import library yang digunakan dalam proyek ini
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""## Data Loading

Menampilkan dataset yang akan digunakan pada proyek ini
"""

df = pd.read_csv('Laptop_price.csv')
df

"""## Exploratory Data Analysis

Melakukan proses investigasi awal pada data untuk menganalisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi pada data.

### Deskripsi Variabel

Menampilkan informasi dari variabel-variabel pada dataset Laptop Price
"""

df.info()

"""Tipe data dari variabel dataset sudah benar

---

Menampilkan deskripsi dari variabel-variabel numerik pada dataset Laptop Price
"""

df.describe()

"""### Missing Values

Mengecek missing value pada dataset
"""

df.isnull().sum()

"""Tidak ditemukan adanya missing value

### Duplicated Data

Mengecek duplicated data pada dataset
"""

df.duplicated().sum()

"""Tidak ditemukan duplicate data

### Outliers

Mengecek kemungkinan adanya outliner pada dataset
"""

numerical_cols = ['Processor_Speed', 'RAM_Size', 'Storage_Capacity', 'Screen_Size', 'Weight', 'Price']

# Menampilkan boxplot untuk tiap kolom
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(x=df[col], color='skyblue')
    plt.title(f'Boxplot of {col}')
    plt.tight_layout()

plt.show()

"""Melakukan pembersihan pada outliner yang dirasa ada"""

# Ambil hanya kolom numerikal
numeric_cols = df.select_dtypes(include='number').columns
# Hitung Q1, Q3, dan IQR hanya untuk kolom numerikal
Q1 = df[numeric_cols].quantile(0.25)
Q3 = df[numeric_cols].quantile(0.75)
IQR = Q3 - Q1
# Buat filter untuk menghapus baris yang mengandung outlier di kolom numerikal
filter_outliers = ~((df[numeric_cols] < (Q1 - 1.5 * IQR)) |
                    (df[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)
# Terapkan filter ke dataset asli (termasuk kolom non-numerikal)
df = df[filter_outliers]
# Cek ukuran dataset setelah outlier dihapus
df.shape

"""Namun ternyata dataset bebas dari adanya outliner

### Univariate Analysis

Melakukan analisis data yang menguji satu variabel secara individu, tanpa menghubungkannya dengan variabel lain
"""

numerical_features = ['Processor_Speed', 'RAM_Size',	'Storage_Capacity',	'Screen_Size',	'Weight',	'Price']
categorical_features = ['Brand']

"""#### Categorical Features

Menampilkan jumlah sampel yang lalu dijadikan histogram pada dataset yang berada di kolom Brand
"""

feature = categorical_features[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
categorical_df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(categorical_df)
count.plot(kind='bar', title=feature);

"""Semua kategori sudah sangat balance dan tidak memiliki kategori yang dominan

#### Numerical Features

Menampilkan histogram variabel-variabel numerik
"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""Histogram menunjukkan bahwa sebagian besar fitur seperti Processor_Speed, Screen_Size, dan Weight memiliki distribusi kontinu yang merata, sedangkan RAM_Size, Storage_Capacity, dan Price bersifat diskrit dengan pola multimodal atau terbatas pada nilai-nilai tertentu.

### Multivariate Analysis

Menampilkan hubungan antara dua atau lebih variabel pada data

#### Categorical Features

Menampilkan rata-rata harga terhadap masing-masing fitur untuk mengetahui pengaruh fitur kategori terhadap harga.
"""

cat_features = df.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="Price", kind="bar", dodge=False, height = 4, aspect = 3,  data=df, palette="Set3")
  plt.title("Rata-rata 'Price' Relatif terhadap - {}".format(col))

"""#### Numerical Features

Mengamati hubungan antara fitur numerik menggunakan fungsi pairplot()
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df, diag_kind = 'kde')

"""Menampilkan Correlation Matrix untuk fitur numerik"""

plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Correlation matrix menunjukkan bahwa hanya ada satu hubungan linear yang kuat antara fitur numerik dan harga (Storage_Capacity), namun sisanya pada nilai korelasi di sekitar nol.

## Data Preparation

Melakukan proses transformasi pada data sehingga menjadi bentuk yang cocok untuk proses pemodelan

### Encoding Fitur Kategori
"""

from sklearn.preprocessing import  OneHotEncoder

df = pd.concat([df, pd.get_dummies(df['Brand'], prefix='Brand', dtype=int)],axis=1)
df.drop(['Brand'], axis=1, inplace=True)
df.head()

"""Menjadikan variabel Brand menjadi numerik agar dapat melakukan pemprosesan model

### Train-Test-Split

Membagi dataset menjadi data latih (train) dan data uji (test)
"""

from sklearn.model_selection import train_test_split

X = df.drop(["Price"],axis =1)
y = df["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""Melakukan split dengan 90:10

## Model Development

Mengembangkan model machine learning dengan tiga algoritma
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'LinearRegression'])

"""### K-Nearest Neighbor (KNN)

KNN bekerja dengan membandingkan jarak satu sampel ke sampel pelatihan lain dengan memilih sejumlah k tetangga terdekat (dengan k adalah sebuah angka positif).

##### GridSearchCV KNN
"""

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

# Menentukan grid parameter untuk KNN
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]  # p=1 untuk Manhattan, p=2 untuk Euclidean
}

# Inisialisasi model KNN
knn = KNeighborsRegressor()

# Inisialisasi GridSearchCV
grid_knn = GridSearchCV(
    estimator=knn,
    param_grid=param_grid_knn,
    cv=5,  # 5-fold cross validation
    scoring='neg_mean_squared_error',  # gunakan MSE sebagai metrik evaluasi
    n_jobs=-1,  # gunakan semua CPU cores
    verbose=1  # tampilkan progres
)

# Melatih GridSearchCV pada data training
grid_knn.fit(X_train, y_train)

# Menampilkan hasil terbaik
print("Best KNN Parameters:", grid_knn.best_params_)
print("Best KNN MSE (train):", -grid_knn.best_score_)

# Menghitung MSE pada data test menggunakan model terbaik
y_pred_test = grid_knn.predict(X_test)
test_mse = mean_squared_error(y_test, y_pred_test)
print("Best KNN MSE (test):", test_mse)

"""Membuat permodelan dengan rekomendasi parameter"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=11, p=2)
knn.fit(X_train, y_train)

models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""### Random Forest

Random Forest adalah model prediksi yang terdiri dari beberapa model dan bekerja secara bersama-sama

##### GridSearchCV Random Forest
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Setup parameter grid
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Inisialisasi model
rf = RandomForestRegressor(random_state=55)

# Grid search
grid_rf = GridSearchCV(rf, param_grid=param_grid_rf, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_rf.fit(X_train, y_train)

# Gunakan model terbaik
best_rf = grid_rf.best_estimator_

# Hitung MSE
train_mse_rf = mean_squared_error(y_train, best_rf.predict(X_train))
test_mse_rf = mean_squared_error(y_test, best_rf.predict(X_test))

print("Best RF Params:", grid_rf.best_params_)
print("Train MSE (RF):", train_mse_rf)
print("Test MSE (RF):", test_mse_rf)

from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error

    # buat model prediksi
    RF = RandomForestRegressor(
        n_estimators=100,
        max_depth= 10,
        min_samples_leaf=4,
        min_samples_split=10
    )
    RF.fit(X_train, y_train)

    models.loc['train_mse', 'RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""Membuat permodelan dengan rekomendasi parameter

### Linear Regression

Linear regression model adalah metode statistik yang digunakan untuk mengukur hubungan linier antara dua atau lebih variabel.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Inisialisasi dan latih model Linear Regression
LR = LinearRegression()
LR.fit(X_train, y_train)

# Simpan hasil MSE train ke dalam DataFrame models
models.loc['train_mse', 'LinearRegression'] = mean_squared_error(y_pred=LR.predict(X_train), y_true=y_train)

"""## Evaluasi Model

### Mean Squared Error (MSE)

MSE atau Mean Squared Error menghitung jumlah selisih kuadrat rata-rata nilai sebenarnya dengan nilai prediksi
"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF', 'LR'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'LR': LR}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

"""Penujian prediksi menggunakan beberapa harga dari data test."""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Histogram dari Mean Squared Error (MSE)"""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""### R² Score (R-squared)

R² Score (R-squared) untuk mengukur seberapa baik model sesuai dengan data, dan seberapa baik model dapat memprediksi hasil di masa mendatang.
"""

from sklearn.metrics import r2_score

# Buat DataFrame untuk menyimpan R² score
r2 = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','LR'])

# Hitung R² score untuk masing-masing model
r2.loc['KNN', 'train'] = r2_score(y_train, knn.predict(X_train))
r2.loc['KNN', 'test'] = r2_score(y_test, knn.predict(X_test))

r2.loc['RF', 'train'] = r2_score(y_train, RF.predict(X_train))
r2.loc['RF', 'test'] = r2_score(y_test, RF.predict(X_test))

r2.loc['LR', 'train'] = r2_score(y_train, LR.predict(X_train))
r2.loc['LR', 'test'] = r2_score(y_test, LR.predict(X_test))

# Tampilkan hasil
r2

"""Histogram dari R² Score (R-squared)"""

fig, ax = plt.subplots()
r2.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Histogram dari kedua teknik evaluasi, dengan R² Score (R-squared)"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Data MSE dan R2 dari hasil sebelumnya
mse_data = {
    'Model': ['KNN', 'RF', 'LR'],
    'Train MSE': [41.160492, 23.52535, 38.921623],
    'Test MSE': [37.756777, 35.608642, 32.365056]
}

r2_data = {
    'Model': ['KNN', 'RF', 'LR'],
    'Train R2': [0.999541, 0.999738, 0.999566],
    'Test R2': [0.999502, 0.999531, 0.999573]
}

mse_df = pd.DataFrame(mse_data)
r2_df = pd.DataFrame(r2_data)

# Plotting
plt.figure(figsize=(16, 6))

# Subplot MSE
plt.subplot(1, 2, 1)
mse_df_melted = mse_df.melt(id_vars='Model', var_name='Dataset', value_name='MSE')
sns.barplot(data=mse_df_melted, x='Model', y='MSE', hue='Dataset', palette='Set2')
plt.title('Perbandingan MSE antar Model')
plt.ylabel('Mean Squared Error')

# Subplot R2
plt.subplot(1, 2, 2)
r2_df_melted = r2_df.melt(id_vars='Model', var_name='Dataset', value_name='R2 Score')
sns.barplot(data=r2_df_melted, x='Model', y='R2 Score', hue='Dataset', palette='Set1')
plt.title('Perbandingan R² antar Model')
plt.ylabel('R² Score')

plt.tight_layout()
plt.show()

"""Linear Regression memberikan generalisasi terbaik dengan MSE dan R² test tertinggi, sementara Random Forest unggul di data train namun cenderung overfitting, dan KNN cukup baik meski performanya tidak sebaik dua model lainnya."""